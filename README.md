# Data Engineering Challenge ‚Äî Eduardo Espinoza

## üß© Descripci√≥n General

Este proyecto implementa la **soluci√≥n completa del reto t√©cnico de ingenier√≠a de datos**, cumpliendo con todos los requisitos de la **Secci√≥n 1 (API)** y **Secci√≥n 2 (SQL)**, adem√°s del **Bonus Track** de **Testing** y **Containerizaci√≥n**.

La aplicaci√≥n est√° desarrollada con **FastAPI (Python 3.8+)**, permite **migrar datos desde archivos CSV hacia una base de datos PostgreSQL**, y expone endpoints para **consultar m√©tricas solicitadas mediante SQL**.

Incluye:
- Recepci√≥n y carga de CSV con validaciones.
- Inserci√≥n por lotes (hasta 1000 filas por request).
- Endpoints para m√©tricas SQL solicitadas.
- Pruebas autom√°ticas con `pytest`.
- Despliegue en **Render.com** mediante contenedor Docker.

---

## ‚öôÔ∏è Estructura del Proyecto

```
eduardo-espinoza-challenge/
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main.py                  # Punto de entrada FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ database.py          # Configuraci√≥n SQLAlchemy (PostgreSQL)
‚îÇ   ‚îú‚îÄ‚îÄ models/                  # Definici√≥n de tablas ORM
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ingest.py            # Endpoints de departamentos, jobs y empleados
‚îÇ   ‚îú‚îÄ‚îÄ services/                # L√≥gica de validaci√≥n y carga de CSV
‚îÇ   ‚îî‚îÄ‚îÄ tests/                   # Pruebas unitarias
‚îÇ
‚îú‚îÄ‚îÄ Dockerfile                   # Imagen para despliegue en Render
‚îú‚îÄ‚îÄ requirements.txt             # Dependencias de Python
‚îú‚îÄ‚îÄ render.yaml                  # Configuraci√≥n de servicio Render.com
‚îú‚îÄ‚îÄ .env.example                 # Variables de entorno base
‚îî‚îÄ‚îÄ README.md                    # Este archivo
```

---

## üß† Constantes y Validaciones

El sistema usa una constante para controlar la **carga masiva m√°xima de registros**:

```python
MAX_BATCH_SIZE = 1000
```

### Validaciones realizadas:
- Estructura de columnas exacta seg√∫n tipo de entidad (`departments`, `jobs`, `employees`).
- Detecci√≥n de columnas faltantes o con nombres no v√°lidos.
- Validaci√≥n de tipos (`int`, `string`, `datetime` ISO 8601).
- Rechazo de lotes con filas inv√°lidas.
- Prevenci√≥n de duplicados por `id`.

---

## üß© Endpoints Principales

### 1Ô∏è‚É£ **Carga de Datos desde CSV**

| M√©todo | Endpoint | Descripci√≥n |
|--------|-----------|-------------|
| `POST` | `/upload/departments/` | Carga CSV con datos de departamentos. |
| `POST` | `/upload/jobs/` | Carga CSV con datos de puestos. |
| `POST` | `/upload/employees/` | Carga CSV de empleados contratados. Admite hasta 1000 registros. |

**Ejemplos de uso (cURL):**

üìÇ **1. Cargar departamentos**
```bash
curl -X POST "http://localhost:8000/upload/" \
  -H "accept: application/json" \
  -H "Content-Type: multipart/form-data" \
  -F "type=departments" \
  -F "file=@departments.csv;type=text/csv"

üìÇ **2. Cargar puestos (jobs)
```bash
curl -X POST "http://localhost:8000/upload/" \
  -H "accept: application/json" \
  -H "Content-Type: multipart/form-data" \
  -F "type=jobs" \
  -F "file=@jobs.csv;type=text/csv"

üìÇ **3. Cargar empleados contratados
```bash
curl -X POST "http://localhost:8000/upload/" \
  -H "accept: application/json" \
  -H "Content-Type: multipart/form-data" \
  -F "type=hired_employees" \
  -F "file=@hired_employees.csv;type=text/csv"

---

### 2Ô∏è‚É£ **Consultas SQL Solicitadas (Secci√≥n 2 del Challenge)**

#### a. Empleados contratados por trimestre (2021)
**Endpoint:**
```
GET /analytics/hired_per_quarter/
```
**Descripci√≥n:**
Devuelve el n√∫mero de empleados contratados por cada **departamento** y **puesto**, separados por trimestre de 2021.

**Salida esperada:**
| department | job | Q1 | Q2 | Q3 | Q4 |
|-------------|-----|----|----|----|----|
| Staff | Recruiter | 3 | 0 | 7 | 11 |

---

#### b. Departamentos que contrataron por encima del promedio (2021)
**Endpoint:**
```
GET /analytics/departments_above_mean/
```
**Descripci√≥n:**
Lista los departamentos que contrataron **m√°s empleados que el promedio global** de 2021, ordenados de forma descendente.

**Salida esperada:**
| id | department | hired |
|----|-------------|--------|
| 7 | Staff | 45 |
| 9 | Supply Chain | 12 |

---


## üß™ Pruebas Autom√°ticas

Las pruebas se ubican en el directorio `src/tests/` y se dividen en **tres m√≥dulos** principales:

| Archivo | Descripci√≥n |
|----------|-------------|
| `test_upload_departments.py` | Valida la carga correcta de `departments.csv` y el rechazo de archivos con estructura inv√°lida. |
| `test_upload_jobs.py` | Verifica la carga del CSV `jobs.csv`, estructura, tipos de datos y control de duplicados. |
| `test_upload_employees.py` | Eval√∫a el endpoint de carga masiva de `hired_employees.csv`, incluyendo l√≠mites y validaciones de formato. |
| `test_analytics_quarter.py` | Prueba el endpoint `/analytics/hired_per_quarter/`, verificando los resultados trimestrales por departamento y puesto. |
| `test_analytics_above_mean.py` | Valida el endpoint `/analytics/departments_above_mean/` y la correcta comparaci√≥n con el promedio global. |

---

### üìã Detalle de las pruebas

#### 1Ô∏è‚É£ **Pruebas de carga de archivos CSV**

**Archivo:** `test_upload_departments.py`
- ‚úÖ Carga exitosa de `departments.csv` con columnas correctas (`id`, `department`).
- ‚ùå Error por columnas faltantes.
- ‚ùå Error por archivo vac√≠o.
- ‚úÖ Inserci√≥n sin duplicados por `id`.

**Archivo:** `test_upload_jobs.py`
- ‚úÖ Inserta correctamente `jobs.csv` con columnas (`id`, `job`).
- ‚ùå Rechaza CSV con tipo de dato incorrecto (por ejemplo, `id` no num√©rico).
- ‚úÖ Confirma respuesta HTTP 200 y conteo correcto de filas insertadas.

**Archivo:** `test_upload_employees.py`
- ‚úÖ Inserta registros v√°lidos de `hired_employees.csv` (hasta `MAX_BATCH_SIZE` = 1000).
- ‚ùå Rechaza CSV con formato incorrecto de fecha o `department_id` inexistente.
- ‚úÖ Detecta l√≠mite de inserci√≥n: si supera 1000 filas, retorna error `400 Bad Request`.
- ‚úÖ Verifica transacciones por lotes y rollback si ocurre error parcial.

---

#### 2Ô∏è‚É£ **Pruebas de endpoints anal√≠ticos**

**Archivo:** `test_analytics_quarter.py`
- ‚úÖ Consulta `/analytics/hired_per_quarter/` y valida:
  - Que los campos `Q1`, `Q2`, `Q3`, `Q4` existan.
  - Que los resultados est√©n ordenados alfab√©ticamente por `department` y `job`.
  - Que la suma total de contrataciones trimestrales coincida con los datos base.

**Archivo:** `test_analytics_above_mean.py`
- ‚úÖ Ejecuta `/analytics/departments_above_mean/` y comprueba:
  - Que solo aparezcan departamentos con contrataciones > promedio 2021.
  - Que el resultado est√© ordenado de mayor a menor por n√∫mero de contrataciones.
  - Que el formato de respuesta contenga `id`, `department` y `hired`.

---

### üß© Datos de prueba utilizados

Durante las pruebas se usan versiones reducidas de los CSV oficiales del challenge:

- `tests/data/departments_sample.csv`
- `tests/data/jobs_sample.csv`
- `tests/data/hired_employees_sample.csv`

Cada uno contiene entre **5 y 20 registros** de ejemplo con datos consistentes entre tablas.

---

### üß™ Ejecuci√≥n de pruebas

Para ejecutar **todas las pruebas** (unitarias + integradas):

```bash
pytest -v --disable-warnings
```

Para ejecutar solo un grupo espec√≠fico:

```bash
pytest src/tests/test_upload_employees.py -v
pytest src/tests/test_analytics_quarter.py -v
```

---

### üßæ Ejemplo de salida esperada
```
====================== test session starts ======================
collected 25 items

src/tests/test_upload_departments.py .....           [ 20%]
src/tests/test_upload_jobs.py ......                 [ 40%]
src/tests/test_upload_employees.py ..........        [ 80%]
src/tests/test_analytics_quarter.py ....             [ 92%]
src/tests/test_analytics_above_mean.py ..            [100%]

================= 25 passed in 4.31s =============================
```



## üöÄ Ejecuci√≥n local del servidor FastAPI

Si deseas ejecutar la aplicaci√≥n **localmente sin Docker**, sigue los pasos:

### 1Ô∏è‚É£ Crear entorno virtual e instalar dependencias
```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 2Ô∏è‚É£ Configurar variables de entorno
Copia el archivo `.env.example` a `.env` y actualiza las credenciales de base de datos.

### 3Ô∏è‚É£ Iniciar el servidor FastAPI
```bash
uvicorn src.main:app --reload
```

### 4Ô∏è‚É£ Acceder a la documentaci√≥n
Abre tu navegador en:
```
http://localhost:8000/docs
```


## üê≥ Despliegue con Docker (Local)

A continuaci√≥n se detallan los comandos necesarios para compilar, ejecutar, inspeccionar y limpiar el entorno Docker del proyecto **eduardo-espinoza-challenge**.

### 1Ô∏è‚É£ Construir la imagen
Compila la imagen Docker localmente:
```bash
docker build -t eduardo-espinoza-challenge .
```

### 2Ô∏è‚É£ Ejecutar el contenedor
Ejecuta el contenedor en segundo plano mapeando el puerto 8000:
```bash
docker run -d -p 8000:8000 --env-file .env eduardo-espinoza-challenge
```

### 3Ô∏è‚É£ Verificar el estado de las im√°genes y contenedores
Listar todas las im√°genes disponibles:
```bash
docker images
```
Listar todos los contenedores activos:
```bash
docker ps
```
Listar todos los contenedores (incluidos detenidos):
```bash
docker ps -a
```

### 4Ô∏è‚É£ Eliminar im√°genes y contenedores (limpieza total)
Eliminar todos los contenedores detenidos:
```bash
docker container prune -f
```
Eliminar todas las im√°genes locales:
```bash
docker rmi -f $(docker images -q)
```
Eliminar todos los vol√∫menes no utilizados:
```bash
docker volume prune -f
```

### 5Ô∏è‚É£ Compilar y ejecutar todo con un solo comando
Este comando limpia, compila y lanza la aplicaci√≥n en un solo paso:
```bash
docker system prune -af && docker build -t eduardo-espinoza-challenge . && docker run -d -p 8000:8000 --env-file .env eduardo-espinoza-challenge
```

### 6Ô∏è‚É£ Acceder a la API
Una vez iniciado el contenedor, abre en tu navegador:
```
http://localhost:8000/docs
```
### 7Ô∏è‚É£ Detener todos los contenedores en ejecuci√≥n
```bash
docker stop $(docker ps -aq)
```


### 8Ô∏è‚É£ Limpiar completamente el entorno Docker (sin errores si est√° vac√≠o)
```bash
# Elimina contenedores detenidos
docker container prune -f

# Elimina im√°genes no usadas
docker image prune -af

# Elimina vol√∫menes no usados
docker volume prune -f

# Limpieza total (contenedores, im√°genes, redes y vol√∫menes)
docker system prune -af --volumes
```


### 9Ô∏è‚É£ Limpiar todo y reconstruir desde cero
```bash
docker system prune -af && docker build -t eduardo-espinoza-challenge . && docker run -d -p 8000:8000 --env-file .env eduardo-espinoza-challenge
```

### üîé Verificar estado del servicio
```bash
curl http://localhost:8000/docs
```



---

## üßπ Limpieza total de Docker

En caso necesites **borrar absolutamente todas las im√°genes, contenedores, vol√∫menes y redes** (por ejemplo, para reiniciar desde cero tu entorno Docker), sigue los pasos a continuaci√≥n.

> ‚ö†Ô∏è **Advertencia:** Esto eliminar√° *todo* lo que tengas en Docker, incluyendo contenedores, im√°genes y bases de datos.  
> √ösalo solo si deseas hacer una limpieza total del sistema Docker.

### 1Ô∏è‚É£ Detener todos los contenedores
```bash
docker stop $(docker ps -aq) 2>/dev/null
```

### 2Ô∏è‚É£ Eliminar todos los contenedores
```bash
docker rm -f $(docker ps -aq) 2>/dev/null
```

### 3Ô∏è‚É£ Eliminar todas las im√°genes
```bash
docker rmi -f $(docker images -q) 2>/dev/null
```

### 4Ô∏è‚É£ Eliminar todos los vol√∫menes (incluye bases de datos)
```bash
docker volume rm $(docker volume ls -q) 2>/dev/null
```

### 5Ô∏è‚É£ Eliminar todas las redes no usadas
```bash
docker network prune -f
```

### 6Ô∏è‚É£ Limpieza general (todo lo anterior junto)
Puedes hacer todo con un solo comando:
```bash
docker system prune -af --volumes
```

### 7Ô∏è‚É£ Verificar que est√° limpio
```bash
docker ps -a
docker images
docker volume ls
```

Si todos los comandos devuelven vac√≠o, tu sistema Docker qued√≥ completamente limpio ‚úÖ



---

## üß± Construir y Reiniciar con Docker Compose

Si utilizas **Docker Compose** para desplegar tu entorno (FastAPI + PostgreSQL), puedes usar los siguientes comandos:

### üèóÔ∏è Construir todo nuevamente
Compila y lanza los contenedores desde cero:
```bash
docker compose up -d --build
```

### üîÅ Reiniciar todos los servicios
Detiene y vuelve a levantar todo el entorno sin reconstruir im√°genes:
```bash
docker compose down
docker compose up -d
```

Estos comandos garantizan que tanto **FastAPI** como **PostgreSQL** se actualicen correctamente y permanezcan sincronizados.


## ‚òÅÔ∏è Despliegue en Render.com

El proyecto fue configurado para **Render.com** usando los siguientes archivos:

| Archivo | Descripci√≥n |
|----------|-------------|
| `Dockerfile` | Define la imagen base, instala dependencias y expone el puerto 8000. |
| `requirements.txt` | Lista de librer√≠as Python requeridas (FastAPI, SQLAlchemy, psycopg2, pytest). |
| `render.yaml` | Indica el servicio web, runtime Docker y comando de inicio. |

### Ejemplo de `render.yaml`
```yaml
services:
  - type: web
    name: fastapi-app
    env: docker
    plan: free
    region: oregon  # Puedes usar frankfurt o virginia si prefieres
    dockerfilePath: ./Dockerfile
    dockerContext: .
    envVars:
      - key: PYTHONUNBUFFERED
        value: "1"
      - key: PG_HOST
        value: your-db-host.railway.app
      - key: PG_PORT
        value: 5432
      - key: PG_USER
        value: kalito
      - key: PG_PASSWORD
        value: AllMyLoving2025$$
      - key: PG_DB
        value: landing
```

---

## üßæ Ejemplo de `.env`

```bash
DATABASE_URL=postgresql+psycopg2://postgres:postgres@localhost:5432/landing
MAX_BATCH_SIZE=1000
```

---

## üß± Tecnolog√≠as Utilizadas

| Componente | Tecnolog√≠a |
|-------------|-------------|
| Lenguaje | Python 3.8 |
| Framework API | FastAPI |
| ORM | SQLAlchemy |
| Base de Datos | PostgreSQL |
| Testing | pytest |
| Contenedores | Docker |
| Cloud Hosting | Render.com |

---

## üë®‚Äçüíª Autor

**Eduardo Espinoza**  
Data Engineering Technical Lead  
[GitHub: lakehousedataengineer](https://github.com/lakehousedataengineer)

---